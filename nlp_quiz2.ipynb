{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tanuj Singhal - nlp_quiz2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WKIPkrmmUIuh"},"source":["##  Softmax\n","Write softmax to convert vectors into probability distribution <br>\n","1) All the elements of probability distribution should be positive. <br>\n","2) Sum of all the elements of distribution will be 1. <br>\n","\n","$Softmax(x_i) = \n","\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"btXWSnSzUSwL","nbgrader":{"checksum":"28a5e10245b99b8ab2b48da1294931ce","grade":false,"grade_id":"cell-4294a66554d00ee0","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["import math\n","import numpy as np\n","def softmax(vector):\n","  \"\"\"\n","  Input : \n","      vector: np array of floats\n","  Output:\n","      distribution: np array of floats of same size as that of input converted by using softmax\n","  \"\"\"\n","  # YOUR CODE HERE\n","  vector=np.exp(vector)\n","#   print(vector)\n","  s=np.sum(vector)\n","  distribution=[]\n","  for i in vector:\n","    distribution.append(i/s)\n","  return distribution  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"jgSQYlhoUYqm","nbgrader":{"checksum":"8ff6ce9c264971c4fcbae23ee8ee2801","grade":true,"grade_id":"cell-17756755bdef637a","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"26493e01-e7b7-470f-f787-0f5b0f987b83","executionInfo":{"status":"ok","timestamp":1561177009802,"user_tz":-330,"elapsed":977,"user":{"displayName":"Tanuj Singhal","photoUrl":"","userId":"15993466899445923096"}}},"source":["softmax(np.array([1,2]))"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.2689414213699951, 0.7310585786300049]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZxZ2s-atUbGD"},"source":["### Convert to One hot\n","Given a vector containing a probability distribution, convert it to one-hot vector with the max index having 1 and rest of the indices having 0"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"nXonICNLUtHI","nbgrader":{"checksum":"f5705ac653b727a1382d43c8b0316aad","grade":false,"grade_id":"cell-ba97ad83ad5b99e6","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def convert_to_one_hot(p):\n","  \"\"\"\n","  Inputs:\n","    p: numpy array of 1 dimension, probability distribution\n","  Outputs:\n","    oh: one_hot vector corresponding to p\n","  \"\"\"\n","  # YOUR CODE HERE\n","#   from keras.utils import to_categorical\n","  x=np.argmax(p)\n","  oh=np.zeros(len(p))\n","  oh[x]+=1\n","  return oh"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"c2ZuIvAdVNCK","nbgrader":{"checksum":"92856f376c2458b0ef41c939ae27f397","grade":true,"grade_id":"cell-dae6f55da8097488","locked":true,"points":2,"schema_version":1,"solution":false},"colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"707016b4-cd3b-4f9e-d198-34cd7d722dde","executionInfo":{"status":"ok","timestamp":1561177359429,"user_tz":-330,"elapsed":1078,"user":{"displayName":"Tanuj Singhal","photoUrl":"","userId":"15993466899445923096"}}},"source":["convert_to_one_hot([0.6,0.3])"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 0.])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4ZXt3L4zVk2l"},"source":["## Complete analogy \n","\n","**Cosine Similarity:**\n","We can find the similarity in terms of  angle between two vectors. Formally, the Cosine Similarity  is  between two vectors  p  and  q  is defined as:\n","\n","$s = \\frac{p⋅q}{||p||||q||} $, where s∈[−1,1]<br>\n","Implement the function"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"5o7TWJYDVk48","nbgrader":{"checksum":"057c75e1bb9747b6827147dd16add597","grade":false,"grade_id":"cell-c710883418fe64f9","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["import math\n","import numpy as np\n","def cosine_similarity(v1,v2):\n","    \"\"\"\n","    Input:\n","        v1: numpy array \n","        v2: numpy array\n","        \n","    Output:\n","        v: single floating point value\n","        \n","        v = cosine similarity between v1 and v2 = (v1 dot v2)/{||v1||*||v2||}\n","    \"\"\"\n","    # YOUR CODE HERE\n","    v1=np.array(v1)\n","    v2=np.array(v2)\n","    v=(v1.dot(v2))/(np.linalg.norm(v1)*np.linalg.norm(v2))\n","    return v"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"IBRXJTiVVk7F","nbgrader":{"checksum":"ca10ad33f541472c8e6e4ba85c99f401","grade":true,"grade_id":"cell-1ddefb3eb9ea742f","locked":true,"points":3,"schema_version":1,"solution":false},"colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ff5e15c1-6ea5-492e-a2a5-d1d6cb773e2d","executionInfo":{"status":"ok","timestamp":1561177474098,"user_tz":-330,"elapsed":1070,"user":{"displayName":"Tanuj Singhal","photoUrl":"","userId":"15993466899445923096"}}},"source":["cosine_similarity([0.5,0.6],[1.9,-0.8])"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2919029442009323"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ayLS2H-LVk89"},"source":["Consider the words $a, b, c, d$ and their corresponding word vectors $x_a, x_b, x_c, x_d$ such that they have this analogical relationship\n","$a:b :: c:d$ <br>\n","For eg.,<br>\n","Princess: Queen : : Prince : ? <br>\n","Complete the analogy by finding the missing word. <br>\n","To find out missing word \"d\", you need to find the word vector which has maximum cosine similarity with the vector $x_b - x_a + x_c$. The word corresponding to this vector is the word that will complete the analogy. In other words, you need to implment the following function <br>\n","$d = argmax_{x_i \\in \\mathcal{X}} \\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||\\cdot ||x_i||}$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"id":"caAFxm09V2bE","nbgrader":{"checksum":"73bfd294eacef5166359054db16e971d","grade":false,"grade_id":"cell-9e2fdcaab9a983a6","locked":false,"schema_version":1,"solution":true},"colab":{}},"source":["def complete_analogy(a, b, c, word_dict):\n","  \"\"\"\n","  Inputs:\n","    a, b, c: strings, with analogical relationship as described above\n","    word_dict: dictionary, dictionary with keys as words and values as corresponding word vectors\n","  Output:\n","      missing: str, the missing word d\n","  \"\"\"\n","  # YOUR CODE HERE\n","  curr=0.1**(20)\n","  result=\"\"\n","  a1=word_dict[a]\n","  b1=word_dict[b]\n","  c1=word_dict[c]\n","  a1=np.array(a1)\n","  b1=np.array(b1)\n","  c1=np.array(c1)\n","\n","  for k in word_dict.keys():\n","   if (k!=a and k!= b and k!=c):\n","     d1=word_dict[k]\n","     value=((b1-a1+c1).T.dot(d1))/(np.linalg.norm(b1-a1+c1)*np.linalg.norm(d1))\n","     if value > curr:\n","       curr=value\n","       result=k\n","  print(result)\n","  return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"2kwj81EHV3pz","nbgrader":{"checksum":"76ebd769ce21a3f481e4d29a8bdd687e","grade":true,"grade_id":"cell-63c4d958a4192123","locked":true,"points":3,"schema_version":1,"solution":false},"outputId":"c89c09c8-ce31-441a-d029-415c96ee9ba9","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1561178501642,"user_tz":-330,"elapsed":1051,"user":{"displayName":"Tanuj Singhal","photoUrl":"","userId":"15993466899445923096"}}},"source":["word_dict = {'princess':\t[-1.720603,\t-3.560657],\n","             'queen':\t[-0.722603,\t-1.232549],\n","\t           'girl':\t[-2.789075,\t-3.869762],\n","             'king':\t[-0.370373,\t0.576843],\n","            'prince':\t[-1.693504,\t0.719822],\n","            'toy':\t[2.78,\t-0.71],\n","            'lady':\t[-1.693,\t-0.7192],\n","            'student':\t[-1.693504,\t0.719822]}\n","\n","complete_analogy('princess', 'queen', 'prince', word_dict)\n","'''test for complete_analogy'''\n","# def test_complete_analogy():\n","# test_complete_analogy()"],"execution_count":92,"outputs":[{"output_type":"stream","text":["king\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'test for complete_analogy'"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"code","metadata":{"id":"_tLZYo_lcoCm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}